# goit-de-fp


## 1. Building an End-to-End Streaming Pipeline 


Ваша задача:


1. Зчитати дані фізичних показників атлетів за допомогою Spark із MySQL таблиці.


2. Відфільтрувати дані.


3. Зчитати дані з результатами змагань з Kafka-топіку.


4. Об’єднати дані з результатами змагань з Kafka-топіку з біологічними даними з MySQL таблиці.


5. Зробити певні трансформації в даних.


6. Зробити стрим даних (за допомогою функції forEachBatch) у:

    а) вихідний кафка-топік,

    b) базу даних.


Ваша задача:

1. Зчитати дані фізичних показників атлетів за допомогою Spark з MySQL таблиці olympic_dataset.athlete_bio (база даних і Credentials до неї вам будуть надані).



2. Відфільтрувати дані, де показники зросту та ваги є порожніми або не є числами. Можна це зробити на будь-якому етапі вашої програми.



3. Зчитати дані з mysql таблиці athlete_event_results і записати в кафка топік athlete_event_results. Зчитати дані з результатами змагань з Kafka-топіку athlete_event_results. Дані з json-формату необхідно перевести в dataframe-формат, де кожне поле json є окремою колонкою.



4. Об’єднати дані з результатами змагань з Kafka-топіку з біологічними даними з MySQL таблиці за допомогою ключа athlete_id.





5. Знайти середній зріст і вагу атлетів індивідуально для кожного виду спорту, типу медалі або її відсутності, статі, країни (country_noc). Додайте також timestamp, коли розрахунки були зроблені.



6. Зробіть стрим даних (за допомогою функції forEachBatch) у:

а) вихідний кафка-топік,

b) базу даних.